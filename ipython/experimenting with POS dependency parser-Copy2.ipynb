{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLibFolder (folder):\n",
    "    import os, sys\n",
    "    if folder not in sys.path:\n",
    "        sys.path.insert(1, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with POS dependency parser\n",
    "To be able to predict a category out of a sentence/text it is assumed that the POS tags and the dependency tree could have an inpact on the result. Here we investigate that relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "import json\n",
    "url = 'http://localhost:1337/sentence/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample text to try out the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseSentence(sentence):\n",
    "    try:\n",
    "        sentence = request.quote(sentence)\n",
    "        f =  request.urlopen(url + sentence)\n",
    "        res = json.loads(f.read().decode('latin1'))\n",
    "        return res\n",
    "    except:\n",
    "        return {'sentenceData': []}\n",
    "def onlyNounsAndVerbs(data):\n",
    "    return {\n",
    "        'sentenceData': [word for word in data['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')]\n",
    "    }\n",
    "def untilLevel(level, data):\n",
    "    return {\n",
    "        'sentenceData': [word for word in data['sentenceData'] if (int)(word['parent']) <= level]\n",
    "    }\n",
    "def toWordArray(data):\n",
    "    return [word['base_word'] for word in data['sentenceData']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = parseSentence('Han ler mot henne och hela hans ansikte säger att han älskar henne med hela sitt hjärta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "{'sentenceData': [{'identifier': '1', 'word': 'han', 'base_word': 'han', 'tag': 'PN|UTR|SIN|DEF|SUB', 'parent': '2'}, {'identifier': '2', 'word': 'ler', 'base_word': 'le', 'tag': 'VB|PRS|AKT', 'parent': '0'}, {'identifier': '3', 'word': 'mot', 'base_word': 'mot', 'tag': 'PP', 'parent': '2'}, {'identifier': '4', 'word': 'henne', 'base_word': 'hon', 'tag': 'PN|UTR|SIN|DEF|OBJ', 'parent': '3'}, {'identifier': '5', 'word': 'och', 'base_word': 'och', 'tag': 'KN', 'parent': '8'}, {'identifier': '6', 'word': 'hela', 'base_word': 'hel', 'tag': 'JJ|POS|UTR/NEU|SIN|DEF|NOM', 'parent': '8'}, {'identifier': '7', 'word': 'hans', 'base_word': 'hans', 'tag': 'PS|UTR/NEU|SIN/PLU|DEF', 'parent': '8'}, {'identifier': '8', 'word': 'ansikte', 'base_word': 'ansikte', 'tag': 'NN|NEU|SIN|IND|NOM', 'parent': '9'}, {'identifier': '9', 'word': 'säger', 'base_word': 'säga', 'tag': 'VB|PRS|AKT', 'parent': '3'}, {'identifier': '10', 'word': 'att', 'base_word': 'att', 'tag': 'SN', 'parent': '9'}, {'identifier': '11', 'word': 'han', 'base_word': 'han', 'tag': 'PN|UTR|SIN|DEF|SUB', 'parent': '12'}, {'identifier': '12', 'word': 'älskar', 'base_word': 'älska', 'tag': 'VB|PRS|AKT', 'parent': '10'}, {'identifier': '13', 'word': 'henne', 'base_word': 'hon', 'tag': 'PN|UTR|SIN|DEF|OBJ', 'parent': '12'}, {'identifier': '14', 'word': 'med', 'base_word': 'med', 'tag': 'PP', 'parent': '17'}, {'identifier': '15', 'word': 'hela', 'base_word': 'hel', 'tag': 'JJ|POS|UTR/NEU|SIN|DEF|NOM', 'parent': '17'}, {'identifier': '16', 'word': 'sitt', 'base_word': 'sin', 'tag': 'PS|NEU|SIN|DEF', 'parent': '17'}, {'identifier': '17', 'word': 'hjärta', 'base_word': 'hjärta', 'tag': 'NN|NEU|SIN|IND|NOM', 'parent': '12'}]}\n",
      "All words:\n",
      "['han', 'ler', 'mot', 'henne', 'och', 'hela', 'hans', 'ansikte', 'säger', 'att', 'han', 'älskar', 'henne', 'med', 'hela', 'sitt', 'hjärta']\n",
      "Level three data:\n",
      "['han::PN', 'ler::VB', 'mot::PP', 'henne::PN', 'säger::VB']\n",
      "Only nouns and verbs:\n",
      "['ler', 'ansikte', 'säger', 'älskar', 'hjärta']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example filtering\n",
    "print (\"Raw data:\")\n",
    "print (res)\n",
    "print (\"All words:\")\n",
    "print ([word['word'] for word in res['sentenceData']])\n",
    "print (\"Level three data:\")\n",
    "print ([word['word']+ '::' + word['tag'].split('|')[0] for word in res['sentenceData'] if (int)(word['parent']) <= 3])\n",
    "print (\"Only nouns and verbs:\")\n",
    "print ([word['word'] for word in res['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')])\n",
    "\n",
    "print(\" \".join(toWordArray(untilLevel(3, onlyNounsAndVerbs(parseSentence(res))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python35.zip', '/usr/lib/python35.zip/../gensim', '/usr/lib/python35.zip/../gensim', '/usr/lib/python35.zip/../gensim', '/usr/lib/python3.5', '/usr/lib/python3.5/plat-x86_64-linux-gnu', '/usr/lib/python3.5/lib-dynload', '', '/home/johlin/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages', '/home/johlin/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/IPython/extensions', '/home/johlin/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "loadLibFolder('../gensim')\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "import gensim_documents\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import random\n",
    "dotenv.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_per_category = 5000\n",
    "use_cache = False\n",
    "use_all_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "unknown URI scheme 'c' in 'C:\\\\Users\\\\desktop-godesity\\\\Documents\\\\Text2Abstract\\\\gensim\\\\trained-sources\\\\doc2vec_MM_2000a_allc.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5d50eccda62f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtimestep_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DOC2VEC_MODEL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cache\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/tmp_dependency_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \"\"\"\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \"\"\"\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ns_exponent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns_exponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \"\"\"\n\u001b[0;32m-> 1355\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode should be a string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shortcut_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mparsed_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/auto-categorizer/venv/lib/python3.5/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(uri_as_string)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         raise NotImplementedError(\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;34m\"unknown URI scheme %r in %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri_as_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         )\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: unknown URI scheme 'c' in 'C:\\\\Users\\\\desktop-godesity\\\\Documents\\\\Text2Abstract\\\\gensim\\\\trained-sources\\\\doc2vec_MM_2000a_allc.model'"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "words = []\n",
    "timesteps = 30\n",
    "timestep_range = 300\n",
    "model = gensim.models.Doc2Vec.load(dotenv.get('DOC2VEC_MODEL'))\n",
    "\n",
    "if use_cache and os.path.isfile('data/tmp_dependency_data'):\n",
    "    with open('data/tmp_dependency_data_lvl4', 'r', encoding='utf-8', errors='ignore') as tmp_cache_file:\n",
    "        for category in tmp_cache_file:\n",
    "            category = category[:-1]\n",
    "            if category == \"\\n\": continue\n",
    "            if category == 'Allmänt': continue\n",
    "            if category == 'Kultur': category = 'Nöje'\n",
    "            if category not in categories:\n",
    "                print (\"TT\", category)\n",
    "                categories.append(category)\n",
    "            sentVecs = []\n",
    "            while True:\n",
    "                sentence = tmp_cache_file.readline()[:-1]\n",
    "                if sentence == \"\":\n",
    "                    break\n",
    "                _words = sentence.split()\n",
    "                for w in _words:\n",
    "                    if w not in words:\n",
    "                        words.append(w)\n",
    "                \n",
    "                # sentVecs.append(gensim.matutils.unitvec(model.infer_vector(doc_words=sentence.split())))\n",
    "                sentVecs.append([words.index(_words[iw]) + 1 if iw < len(_words) else 0 for iw in range(timestep_range)])\n",
    "            \n",
    "            stepsToAdd = max([0, timesteps - len(sentVecs)])\n",
    "            for i in range(stepsToAdd):\n",
    "                if stepsToAdd <= 0: break\n",
    "                sentVecs.append(np.zeros(300))\n",
    "            y_data.append(categories.index(category))\n",
    "            x_data.append(sentVecs[:timesteps])\n",
    "else:\n",
    "    data = gensim_documents.MMDBDocumentLists(dotenv.get('ARTICLE_PATH', '.') + '/csv_by_category/', useHeading=True, limit=limit_per_category)\n",
    "    tmp_cache_file = None\n",
    "    ids = []\n",
    "    if not use_all_data: tmp_cache_file =  open('data/tmp_dependency_data', 'w', encoding='utf-8', errors='ignore')\n",
    "    for i, doc in enumerate(data):\n",
    "        if doc.category == 'Allmänt': continue\n",
    "        if not doc.category in categories:\n",
    "            categories.append(doc.category)\n",
    "        if doc.pageid in ids: \n",
    "            print(\"copy of article found\", doc.pageid)\n",
    "            continue\n",
    "        else: ids.append(doc.pageid)\n",
    "        if tmp_cache_file != None:\n",
    "            tmp_cache_file.write(doc.category + \"\\n\")\n",
    "\n",
    "        sentences = doc.content.split(\".\")\n",
    "        sentVecs = []\n",
    "        for j in range(timesteps):\n",
    "            if j >= len(sentences): \n",
    "                sentVecs.append(np.zeros(timestep_range))\n",
    "                continue\n",
    "            if use_all_data:\n",
    "                sentence = sentences[j]\n",
    "            else:\n",
    "                sentence = \" \".join(toWordArray(untilLevel(3, onlyNounsAndVerbs(parseSentence(sentences[j])))))\n",
    "            if sentence == \"\":\n",
    "                sentVecs.append(np.zeros(timestep_range))\n",
    "                continue\n",
    "            _words = sentence.split()\n",
    "            for w in _words:\n",
    "                if w not in words:\n",
    "                    words.append(w)\n",
    "            # sentVecs.append(gensim.matutils.unitvec(model.infer_vector(doc_words=_words)))\n",
    "            sentVecs.append([words.index(_words[iw]) + 1 if iw < len(_words) else 0 for iw in range(timestep_range)])\n",
    "            if tmp_cache_file != None:\n",
    "                tmp_cache_file.write(sentence + \"\\n\")\n",
    "        \n",
    "        if tmp_cache_file != None:\n",
    "            tmp_cache_file.write(\"\\n\")\n",
    "        x_data.append(sentVecs)\n",
    "        y_data.append(categories.index(doc.category))\n",
    "\n",
    "        if i % (limit_per_category/4) == 0 and i != 0:\n",
    "            print (\"New epoch started, nr.\", i, \"of\", len(categories) * limit_per_category, \"epochs\", 100 * float(i) / float(len(categories) * limit_per_category), \" %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gensim_documents.MMDBDocumentLists('../MM/csv_by_category/', useHeading=True, limit=5000)\n",
    "articles = [(a.content, a.category) for a in data if a.category != 'Allmänt']\n",
    "random.shuffle(articles)\n",
    "categories = list(set(list(zip(*articles))[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = gensim.models.Doc2Vec.load('../gensim/trained-sources/doc2vec_MM_14000a_original_linear_allc.model')\n",
    "articles_labels, articles_vectors = zip(*[\n",
    "    (categories.index(article[1]),\n",
    "      [gensim.matutils.unitvec(model.infer_vector(doc_words=sentence.split(' ')))\n",
    "       for sentence in article[0].split('.')]\n",
    "    ) for article in articles\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 30\n",
    "articles_fixed_vectors = [\n",
    "    [article[i] if len(article) > i else np.zeros(len(articles_vectors[0][0])) \n",
    "     for i in range(timesteps)] \n",
    "    for article in articles_vectors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = articles_labels\n",
    "x_data = articles_fixed_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode one hot vectors for the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_one_hot = np.zeros((len(y_data), len(categories)))\n",
    "y_data_one_hot[np.arange(len(y_data)), np.array(y_data)] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM classification with keras LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = len(x_data[0][0])\n",
    "timesteps = len(x_data[0])\n",
    "num_classes = len(categories)\n",
    "n_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "limit_train = (int)(len(x_data) * split)\n",
    "# Generate dummy training data\n",
    "x_train = x_data[:limit_train]\n",
    "y_train = y_data_one_hot[:limit_train]\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = x_data[limit_train:]\n",
    "y_val = y_data_one_hot[limit_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n",
      "30\n",
      "100\n",
      "2400\n",
      "30\n",
      "100\n",
      "6\n",
      "[[0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_val))\n",
    "print(len(x_val[0]))\n",
    "print(len(x_val[0][0]))\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_train[0]))\n",
    "print(len(x_train[0][0]))\n",
    "print(len(categories))\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Sequential LSTM model that can classify a stacked sequence of words we need to define the input as follows:\n",
    " * batch_size - number of datapoints in the dataset\n",
    " * timesteps - the number of words per sequence\n",
    " * data_dim - the number of features per word instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 30, 100) (9600, 30, 100)\n",
      "(30, 100) 6\n",
      "Train on 2400 samples, validate on 9600 samples\n",
      "Epoch 1/15\n",
      "2400/2400 [==============================] - 10s 4ms/step - loss: 1.6830 - acc: 0.2733 - val_loss: 1.6548 - val_acc: 0.2922\n",
      "Epoch 2/15\n",
      "2400/2400 [==============================] - 8s 3ms/step - loss: 1.6161 - acc: 0.3129 - val_loss: 1.6019 - val_acc: 0.2995\n",
      "Epoch 3/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.5593 - acc: 0.3117 - val_loss: 1.5287 - val_acc: 0.3328\n",
      "Epoch 4/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.5053 - acc: 0.3471 - val_loss: 1.4680 - val_acc: 0.3642\n",
      "Epoch 5/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.4572 - acc: 0.3779 - val_loss: 1.4595 - val_acc: 0.3744\n",
      "Epoch 6/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.4050 - acc: 0.4150 - val_loss: 1.6664 - val_acc: 0.3058\n",
      "Epoch 7/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.3522 - acc: 0.4325 - val_loss: 1.3843 - val_acc: 0.4339\n",
      "Epoch 8/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.2990 - acc: 0.4825 - val_loss: 1.3383 - val_acc: 0.4576\n",
      "Epoch 9/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.2590 - acc: 0.4967 - val_loss: 1.3708 - val_acc: 0.4506\n",
      "Epoch 10/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.2224 - acc: 0.5158 - val_loss: 1.3691 - val_acc: 0.4506\n",
      "Epoch 11/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.1945 - acc: 0.5492 - val_loss: 1.5355 - val_acc: 0.4140\n",
      "Epoch 12/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.1748 - acc: 0.5442 - val_loss: 1.3336 - val_acc: 0.4889\n",
      "Epoch 13/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.1509 - acc: 0.5604 - val_loss: 1.2162 - val_acc: 0.5241\n",
      "Epoch 14/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.1209 - acc: 0.5700 - val_loss: 1.1941 - val_acc: 0.5447\n",
      "Epoch 15/15\n",
      "2400/2400 [==============================] - 9s 4ms/step - loss: 1.1006 - acc: 0.5758 - val_loss: 1.2433 - val_acc: 0.5093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2ef6e7470>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "input_shape = (timesteps, data_dim,)\n",
    "print(np.array(x_train).shape,np.array(x_val).shape)\n",
    "print(input_shape, num_classes)\n",
    "model.add(LSTM(50, return_sequences=True,\n",
    "               input_shape=input_shape))  # returns a sequence of vectors of dimension 32\n",
    "for layer in range(n_layers-2):\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(50))  # return a single vector of dimension 32\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([x_train], [y_train], epochs=15, validation_data=([x_val], [y_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(np.array(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_prediction = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_proba = min([max(y) for y in copy_prediction])\n",
    "\n",
    "mean_proba = sum([sum(y) for y in copy_prediction]) / (len(copy_prediction) * len(copy_prediction[0]))\n",
    "\n",
    "len([y for y in copy_prediction if max(y) > mean_proba])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat all predictions above min_proba/avg_proba probability as correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_prediction = [[1.0 if proba in [x for x in prediction_instance if (x >= min_proba and prediction_instance.tolist().index(x) == y_val[index].tolist().index(1.0))] \\\n",
    "                    else 0.0 \\\n",
    "                        for proba in prediction_instance] \\\n",
    "                    for index, prediction_instance in enumerate(prediction)]\n",
    "copy_prediction = [[1.0 if prediction[i].tolist().index(max(prediction[i])) == j or proba == 1.0 else 0.0 \\\n",
    "                        for j, proba in enumerate(prediction_instance)] \\\n",
    "                   for i, prediction_instance in enumerate(copy_prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.614375"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for index, x in enumerate(copy_prediction)  if (x.index(1.0) == y_val[index].tolist().index(1.0))])/len(copy_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix([categories[y.argmax()] for y in y_val], [categories[y.argmax()] for y in np.array(copy_prediction)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Släkt o vänner</th>\n",
       "      <th>Nöje</th>\n",
       "      <th>Ekonomi</th>\n",
       "      <th>Blåljus</th>\n",
       "      <th>Kultur</th>\n",
       "      <th>Sport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Släkt o vänner</th>\n",
       "      <td>982</td>\n",
       "      <td>97</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "      <td>420</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nöje</th>\n",
       "      <td>94</td>\n",
       "      <td>1047</td>\n",
       "      <td>98</td>\n",
       "      <td>43</td>\n",
       "      <td>305</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ekonomi</th>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>1091</td>\n",
       "      <td>93</td>\n",
       "      <td>288</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blåljus</th>\n",
       "      <td>118</td>\n",
       "      <td>65</td>\n",
       "      <td>145</td>\n",
       "      <td>971</td>\n",
       "      <td>300</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kultur</th>\n",
       "      <td>49</td>\n",
       "      <td>91</td>\n",
       "      <td>123</td>\n",
       "      <td>34</td>\n",
       "      <td>1300</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>649</td>\n",
       "      <td>98</td>\n",
       "      <td>23</td>\n",
       "      <td>157</td>\n",
       "      <td>161</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Släkt o vänner  Nöje  Ekonomi  Blåljus  Kultur  Sport\n",
       "Släkt o vänner             982    97       47       37     420      4\n",
       "Nöje                        94  1047       98       43     305     13\n",
       "Ekonomi                     45    74     1091       93     288      7\n",
       "Blåljus                    118    65      145      971     300     14\n",
       "Kultur                      49    91      123       34    1300     10\n",
       "Sport                      649    98       23      157     161    507"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(conf_mat, columns=categories, index=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Släkt o vänner</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nöje</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ekonomi</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blåljus</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kultur</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sport</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Count\n",
       "Släkt o vänner      0\n",
       "Nöje                0\n",
       "Ekonomi             0\n",
       "Blåljus             0\n",
       "Kultur              0\n",
       "Sport               0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat = confusion_matrix([categories[y.argmax()] for y in y_val], [categories[0] for y in y_val])\n",
    "tmp_pd = pandas.DataFrame(conf_mat, columns=['Count'] + categories[1:], index=categories)\n",
    "pandas.DataFrame(tmp_pd['Count'], columns=['Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "2018-05-22 00:16:12,180 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-05-22 00:16:12,181 : INFO : collecting all words and their counts\n",
      "2018-05-22 00:16:12,182 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-22 00:16:12,721 : INFO : PROGRESS: at sentence #10000, processed 2257122 words, keeping 135772 word types\n",
      "2018-05-22 00:16:12,936 : INFO : collected 167118 word types from a corpus of 3173357 raw words and 14000 sentences\n",
      "2018-05-22 00:16:12,937 : INFO : Loading a fresh vocabulary\n",
      "2018-05-22 00:16:13,138 : INFO : min_count=5 retains 34825 unique words (20% of original 167118, drops 132293)\n",
      "2018-05-22 00:16:13,139 : INFO : min_count=5 leaves 2972284 word corpus (93% of original 3173357, drops 201073)\n",
      "2018-05-22 00:16:13,235 : INFO : deleting the raw counts dictionary of 167118 items\n",
      "2018-05-22 00:16:13,240 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2018-05-22 00:16:13,241 : INFO : downsampling leaves estimated 2281922 word corpus (76.8% of prior 2972284)\n",
      "2018-05-22 00:16:13,242 : INFO : estimated required memory for 34825 words and 250 dimensions: 87062500 bytes\n",
      "2018-05-22 00:16:13,372 : INFO : resetting layer weights\n",
      "2018-05-22 00:16:13,890 : INFO : training model with 3 workers on 34825 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2018-05-22 00:16:14,900 : INFO : PROGRESS: at 7.56% examples, 854122 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:15,904 : INFO : PROGRESS: at 14.74% examples, 834584 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:16,913 : INFO : PROGRESS: at 21.79% examples, 821158 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:17,920 : INFO : PROGRESS: at 27.68% examples, 783460 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:18,926 : INFO : PROGRESS: at 35.48% examples, 803474 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:19,930 : INFO : PROGRESS: at 42.84% examples, 808158 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:20,943 : INFO : PROGRESS: at 49.84% examples, 807059 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:21,950 : INFO : PROGRESS: at 57.57% examples, 814908 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:22,951 : INFO : PROGRESS: at 63.21% examples, 795410 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:23,952 : INFO : PROGRESS: at 68.94% examples, 782083 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:24,957 : INFO : PROGRESS: at 76.10% examples, 783763 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:25,970 : INFO : PROGRESS: at 83.09% examples, 784348 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:26,978 : INFO : PROGRESS: at 90.51% examples, 789598 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:27,992 : INFO : PROGRESS: at 97.13% examples, 785882 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-22 00:16:28,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-22 00:16:28,406 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-22 00:16:28,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-22 00:16:28,414 : INFO : training on 15866785 raw words (11409406 effective words) took 14.5s, 785718 effective words/s\n",
      "2018-05-22 00:16:28,416 : INFO : saving Word2Vec object under ../gensim/trained-sources/word2vec_MM_180521.model, separately None\n",
      "2018-05-22 00:16:28,418 : INFO : not storing attribute syn0norm\n",
      "2018-05-22 00:16:28,419 : INFO : not storing attribute cum_table\n",
      "2018-05-22 00:16:29,354 : INFO : saved ../gensim/trained-sources/word2vec_MM_180521.model\n"
     ]
    }
   ],
   "source": [
    "loadLibFolder('../gensim')\n",
    "import word2vec_train\n",
    "word2vec_train.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.keyedvectors' from 'C:\\\\Users\\\\desktop-godesity\\\\Anaconda3\\\\lib\\\\site-packages\\\\gensim\\\\models\\\\keyedvectors.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e6a59daf9a74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloadLibFolder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../gensim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mCNN_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\desktop-godesity\\Documents\\Text2Abstract\\gensim\\CNN_test.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Load word2vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mw2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../gensim/trained-sources/word2vec_MM_180521.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m#d2v_model = gensim.models.Doc2Vec.load('doc2vec.model')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1410\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1412\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1413\u001b[0m         \u001b[1;31m# update older models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m    936\u001b[0m         \u001b[1;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'Word2VecKeyedVectors' on <module 'gensim.models.keyedvectors' from 'C:\\\\Users\\\\desktop-godesity\\\\Anaconda3\\\\lib\\\\site-packages\\\\gensim\\\\models\\\\keyedvectors.py'>"
     ]
    }
   ],
   "source": [
    "loadLibFolder('../gensim')\n",
    "import CNN_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-23 09:06:54,721 : INFO : loading Word2Vec object from ../gensim/trained-sources/word2vec_MM_180521.model\n",
      "2018-05-23 09:06:55,524 : INFO : loading wv recursively from ../gensim/trained-sources/word2vec_MM_180521.model.wv.* with mmap=None\n",
      "2018-05-23 09:06:55,525 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-05-23 09:06:55,526 : INFO : setting ignored attribute cum_table to None\n",
      "2018-05-23 09:06:55,527 : INFO : loaded ../gensim/trained-sources/word2vec_MM_180521.model\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "d2v_model = gensim.models.Word2Vec.load('../gensim/trained-sources/word2vec_MM_180521.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'closer_than'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6ec3cb5acec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloser_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sundsvall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Stockholm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Karlstad'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bostad'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#d2v_model.vocab.keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'closer_than'"
     ]
    }
   ],
   "source": [
    "d2v_model.wv.closer_than(['Sundsvall', 'Stockholm', 'Karlstad', 'bostad']) #d2v_model.vocab.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'most_similar_to_given'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-10ad3d8f5164>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar_to_given\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sundsvall'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Mittuniversitetet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MittMedia'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyedVectors' object has no attribute 'most_similar_to_given'"
     ]
    }
   ],
   "source": [
    "d2v_model.wv.most_similar_to_given('Sundsvall', 'Mittuniversitetet', 'MittMedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5f2f3df2b775>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "dir(d2v_model.wv.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
