{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadLibFolder (folder):\n",
    "    import os, sys\n",
    "    if folder not in sys.path:\n",
    "        sys.path.insert(1, os.path.join(sys.path[0], folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with POS dependency parser\n",
    "To be able to predict a category out of a sentence/text it is assumed that the POS tags and the dependency tree could have an inpact on the result. Here we investigate that relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "import json\n",
    "url = 'http://localhost:1337/sentence/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample text to try out the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseSentence(sentence):\n",
    "    try:\n",
    "        sentence = request.quote(sentence)\n",
    "        f =  request.urlopen(url + sentence)\n",
    "        res = json.loads(f.read().decode('latin1'))\n",
    "        return res\n",
    "    except:\n",
    "        return {'sentenceData': []}\n",
    "def onlyNounsAndVerbs(data):\n",
    "    return {\n",
    "        'sentenceData': [word for word in data['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')]\n",
    "    }\n",
    "def untilLevel(level, data):\n",
    "    return {\n",
    "        'sentenceData': [word for word in data['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')]\n",
    "    }\n",
    "def toWordArray(data):\n",
    "    return [word['word'] for word in data['sentenceData']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = parseSentence('Han ler mot henne och hela hans ansikte säger att han älskar henne med hela sitt hjärta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "{'sentenceData': [{'identifier': '1', 'word': 'han', 'base_word': 'han', 'tag': 'PN|UTR|SIN|DEF|SUB', 'parent': '2'}, {'identifier': '2', 'word': 'ler', 'base_word': 'le', 'tag': 'VB|PRS|AKT', 'parent': '0'}, {'identifier': '3', 'word': 'mot', 'base_word': 'mot', 'tag': 'PP', 'parent': '2'}, {'identifier': '4', 'word': 'henne', 'base_word': 'hon', 'tag': 'PN|UTR|SIN|DEF|OBJ', 'parent': '3'}, {'identifier': '5', 'word': 'och', 'base_word': 'och', 'tag': 'KN', 'parent': '8'}, {'identifier': '6', 'word': 'hela', 'base_word': 'hel', 'tag': 'JJ|POS|UTR/NEU|SIN|DEF|NOM', 'parent': '8'}, {'identifier': '7', 'word': 'hans', 'base_word': 'hans', 'tag': 'PS|UTR/NEU|SIN/PLU|DEF', 'parent': '8'}, {'identifier': '8', 'word': 'ansikte', 'base_word': 'ansikte', 'tag': 'NN|NEU|SIN|IND|NOM', 'parent': '9'}, {'identifier': '9', 'word': 'säger', 'base_word': 'säga', 'tag': 'VB|PRS|AKT', 'parent': '3'}, {'identifier': '10', 'word': 'att', 'base_word': 'att', 'tag': 'SN', 'parent': '9'}, {'identifier': '11', 'word': 'han', 'base_word': 'han', 'tag': 'PN|UTR|SIN|DEF|SUB', 'parent': '12'}, {'identifier': '12', 'word': 'älskar', 'base_word': 'älska', 'tag': 'VB|PRS|AKT', 'parent': '10'}, {'identifier': '13', 'word': 'henne', 'base_word': 'hon', 'tag': 'PN|UTR|SIN|DEF|OBJ', 'parent': '12'}, {'identifier': '14', 'word': 'med', 'base_word': 'med', 'tag': 'PP', 'parent': '17'}, {'identifier': '15', 'word': 'hela', 'base_word': 'hel', 'tag': 'JJ|POS|UTR/NEU|SIN|DEF|NOM', 'parent': '17'}, {'identifier': '16', 'word': 'sitt', 'base_word': 'sin', 'tag': 'PS|NEU|SIN|DEF', 'parent': '17'}, {'identifier': '17', 'word': 'hjärta', 'base_word': 'hjärta', 'tag': 'NN|NEU|SIN|IND|NOM', 'parent': '12'}]}\n",
      "All words:\n",
      "['han', 'ler', 'mot', 'henne', 'och', 'hela', 'hans', 'ansikte', 'säger', 'att', 'han', 'älskar', 'henne', 'med', 'hela', 'sitt', 'hjärta']\n",
      "Level three data:\n",
      "['han::PN', 'ler::VB', 'mot::PP', 'henne::PN', 'säger::VB']\n",
      "Only nouns and verbs:\n",
      "['ler', 'ansikte', 'säger', 'älskar', 'hjärta']\n"
     ]
    }
   ],
   "source": [
    "# Example filtering\n",
    "print (\"Raw data:\")\n",
    "print (res)\n",
    "print (\"All words:\")\n",
    "print ([word['word'] for word in res['sentenceData']])\n",
    "print (\"Level three data:\")\n",
    "print ([word['word']+ '::' + word['tag'].split('|')[0] for word in res['sentenceData'] if (int)(word['parent']) <= 3])\n",
    "print (\"Only nouns and verbs:\")\n",
    "print ([word['word'] for word in res['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "loadLibFolder('../gensim')\n",
    "\n",
    "import gensim_documents\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gensim_documents.MMDBDocumentLists(dotenv.get('ARTICLE_PATH', '.') + '/csv_by_category_uuid-filter/', useHeading=True, limit=1000)\n",
    "dictionary = gensim_documents.VectorDictionary()\n",
    "for doc in data: dictionary.addToDictionary(\" \".join(untilLevel(3, onlyNounsAndVerbs(parseSentence(doc.content.split(\".\")[0])))), doc.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM classification with keras LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim = len(dictionary[0])\n",
    "timesteps = 8\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate dummy training data\n",
    "x_train = np.random.random((3000, timesteps, data_dim))\n",
    "y_train = np.random.random((3000, num_classes))\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = np.random.random((2000, timesteps, data_dim))\n",
    "y_val = np.random.random((2000, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_val[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Sequential LSTM model that can classify a stacked sequence of words we need to define the input as follows:\n",
    " * batch_size - number of datapoints in the dataset\n",
    " * timesteps - the number of words per sequence\n",
    " * data_dim - the number of features per word instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "3000/3000 [==============================] - 4s - loss: 11.4635 - acc: 0.0973 - val_loss: 11.5334 - val_acc: 0.0975\n",
      "Epoch 2/5\n",
      "3000/3000 [==============================] - 1s - loss: 11.4628 - acc: 0.0940 - val_loss: 11.5325 - val_acc: 0.1125\n",
      "Epoch 3/5\n",
      "3000/3000 [==============================] - 1s - loss: 11.4623 - acc: 0.1017 - val_loss: 11.5327 - val_acc: 0.0900\n",
      "Epoch 4/5\n",
      "3000/3000 [==============================] - 1s - loss: 11.4621 - acc: 0.0987 - val_loss: 11.5332 - val_acc: 0.0995\n",
      "Epoch 5/5\n",
      "3000/3000 [==============================] - 1s - loss: 11.4619 - acc: 0.1007 - val_loss: 11.5327 - val_acc: 0.1045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27517935908>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1792/2000 [=========================>....] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.532712387084961, 0.1045]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.09628296,  0.67249097,  0.4890543 , ...,  0.29885446,\n",
       "          0.89960575,  0.19898042],\n",
       "        [ 0.94419746,  0.37159193,  0.47884219, ...,  0.86476085,\n",
       "          0.78584042,  0.34626552],\n",
       "        [ 0.55632149,  0.39933993,  0.77890599, ...,  0.0398117 ,\n",
       "          0.98907483,  0.90146782],\n",
       "        ..., \n",
       "        [ 0.20993563,  0.44732187,  0.23160779, ...,  0.56499002,\n",
       "          0.32134186,  0.18139511],\n",
       "        [ 0.67799434,  0.34190327,  0.13457328, ...,  0.3103999 ,\n",
       "          0.07947997,  0.92366944],\n",
       "        [ 0.5702733 ,  0.34225729,  0.81988819, ...,  0.05292903,\n",
       "          0.37115567,  0.76821646]],\n",
       "\n",
       "       [[ 0.00983867,  0.73681855,  0.29397279, ...,  0.41512319,\n",
       "          0.36352249,  0.20193833],\n",
       "        [ 0.58469941,  0.54892885,  0.74738472, ...,  0.83592186,\n",
       "          0.92938897,  0.04370301],\n",
       "        [ 0.28451859,  0.93110384,  0.07664203, ...,  0.12766366,\n",
       "          0.50895263,  0.12491126],\n",
       "        ..., \n",
       "        [ 0.47032118,  0.39299207,  0.89978608, ...,  0.11120168,\n",
       "          0.88720519,  0.18499065],\n",
       "        [ 0.86174532,  0.18883529,  0.32357516, ...,  0.01412559,\n",
       "          0.66340271,  0.03477711],\n",
       "        [ 0.5569859 ,  0.83812081,  0.9020118 , ...,  0.94957241,\n",
       "          0.55244848,  0.11622821]],\n",
       "\n",
       "       [[ 0.48220304,  0.86255395,  0.44256332, ...,  0.72223599,\n",
       "          0.5680695 ,  0.83670154],\n",
       "        [ 0.02553666,  0.49796777,  0.40437237, ...,  0.6192616 ,\n",
       "          0.91704799,  0.72222691],\n",
       "        [ 0.58214615,  0.23815252,  0.29854061, ...,  0.207114  ,\n",
       "          0.45102906,  0.07582504],\n",
       "        ..., \n",
       "        [ 0.09536184,  0.20464757,  0.15085617, ...,  0.68238673,\n",
       "          0.27929195,  0.53853113],\n",
       "        [ 0.30471722,  0.20909906,  0.68110944, ...,  0.52026118,\n",
       "          0.17376947,  0.15914788],\n",
       "        [ 0.17311019,  0.09076549,  0.1773296 , ...,  0.45125227,\n",
       "          0.2566307 ,  0.20633844]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.8046144 ,  0.57151152,  0.0937889 , ...,  0.91588217,\n",
       "          0.32264617,  0.25848366],\n",
       "        [ 0.12171016,  0.51174526,  0.71845249, ...,  0.00462168,\n",
       "          0.15691273,  0.13282774],\n",
       "        [ 0.70266789,  0.11643851,  0.17300379, ...,  0.62891653,\n",
       "          0.9187759 ,  0.84010647],\n",
       "        ..., \n",
       "        [ 0.66338716,  0.80189202,  0.63980599, ...,  0.41886653,\n",
       "          0.73131729,  0.02858674],\n",
       "        [ 0.59546938,  0.07508805,  0.13935117, ...,  0.82936863,\n",
       "          0.15732345,  0.94221517],\n",
       "        [ 0.89507985,  0.68886571,  0.53248872, ...,  0.95033234,\n",
       "          0.16030949,  0.03942244]],\n",
       "\n",
       "       [[ 0.71658112,  0.410449  ,  0.0026861 , ...,  0.17338475,\n",
       "          0.50004881,  0.68326474],\n",
       "        [ 0.63437125,  0.91257187,  0.14682858, ...,  0.53896817,\n",
       "          0.78880211,  0.77644364],\n",
       "        [ 0.28440406,  0.50486639,  0.4569193 , ...,  0.50095172,\n",
       "          0.68460816,  0.3090704 ],\n",
       "        ..., \n",
       "        [ 0.35070007,  0.3845061 ,  0.53644285, ...,  0.43731899,\n",
       "          0.81124028,  0.71003689],\n",
       "        [ 0.12418505,  0.70767846,  0.60471497, ...,  0.39723265,\n",
       "          0.78597107,  0.94860716],\n",
       "        [ 0.5024725 ,  0.32936379,  0.71712731, ...,  0.3825955 ,\n",
       "          0.26698164,  0.49027254]],\n",
       "\n",
       "       [[ 0.63416666,  0.84030287,  0.73707304, ...,  0.36039666,\n",
       "          0.20533516,  0.830775  ],\n",
       "        [ 0.73422189,  0.15777188,  0.55716123, ...,  0.6976972 ,\n",
       "          0.10064756,  0.49607446],\n",
       "        [ 0.14340107,  0.04184228,  0.27236275, ...,  0.48608616,\n",
       "          0.39041129,  0.76944757],\n",
       "        ..., \n",
       "        [ 0.86240129,  0.66519017,  0.17464542, ...,  0.36027335,\n",
       "          0.40290623,  0.03466169],\n",
       "        [ 0.09367136,  0.58509189,  0.12540858, ...,  0.43328995,\n",
       "          0.20527968,  0.6324186 ],\n",
       "        [ 0.61012515,  0.75933447,  0.74430494, ...,  0.85770472,\n",
       "          0.17931475,  0.93559732]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-f26b5a6814fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdotenv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ARTICLE_PATH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "dotenv.get('ARTICLE_PATH', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
