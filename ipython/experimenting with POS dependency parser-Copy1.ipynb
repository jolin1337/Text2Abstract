{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadLibFolder (folder):\n",
    "    import os, sys\n",
    "    if folder not in sys.path:\n",
    "        sys.path.insert(1, os.path.join(sys.path[0], folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with POS dependency parser\n",
    "To be able to predict a category out of a sentence/text it is assumed that the POS tags and the dependency tree could have an inpact on the result. Here we investigate that relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "import json\n",
    "url = 'http://localhost:1337/sentence/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample text to try out the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseSentence(sentence):\n",
    "    try:\n",
    "        sentence = request.quote(sentence)\n",
    "        f =  request.urlopen(url + sentence)\n",
    "        res = json.loads(f.read().decode('latin1'))\n",
    "        return res\n",
    "    except:\n",
    "        return {'sentenceData': []}\n",
    "def onlyNounsAndVerbs(data):\n",
    "    return {\n",
    "        'sentenceData': [word for word in data['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')]\n",
    "    }\n",
    "def untilLevel(level, data):\n",
    "    return {\n",
    "        'sentenceData': [word for word in data['sentenceData'] if (int)(word['parent']) <= level]\n",
    "    }\n",
    "def toWordArray(data):\n",
    "    return [word['base_word'] for word in data['sentenceData']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = parseSentence('Han ler mot henne och hela hans ansikte säger att han älskar henne med hela sitt hjärta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data:\n",
      "{'sentenceData': [{'identifier': '1', 'word': 'han', 'base_word': 'han', 'tag': 'PN|UTR|SIN|DEF|SUB', 'parent': '2'}, {'identifier': '2', 'word': 'ler', 'base_word': 'le', 'tag': 'VB|PRS|AKT', 'parent': '0'}, {'identifier': '3', 'word': 'mot', 'base_word': 'mot', 'tag': 'PP', 'parent': '2'}, {'identifier': '4', 'word': 'henne', 'base_word': 'hon', 'tag': 'PN|UTR|SIN|DEF|OBJ', 'parent': '3'}, {'identifier': '5', 'word': 'och', 'base_word': 'och', 'tag': 'KN', 'parent': '8'}, {'identifier': '6', 'word': 'hela', 'base_word': 'hel', 'tag': 'JJ|POS|UTR/NEU|SIN|DEF|NOM', 'parent': '8'}, {'identifier': '7', 'word': 'hans', 'base_word': 'hans', 'tag': 'PS|UTR/NEU|SIN/PLU|DEF', 'parent': '8'}, {'identifier': '8', 'word': 'ansikte', 'base_word': 'ansikte', 'tag': 'NN|NEU|SIN|IND|NOM', 'parent': '9'}, {'identifier': '9', 'word': 'säger', 'base_word': 'säga', 'tag': 'VB|PRS|AKT', 'parent': '3'}, {'identifier': '10', 'word': 'att', 'base_word': 'att', 'tag': 'SN', 'parent': '9'}, {'identifier': '11', 'word': 'han', 'base_word': 'han', 'tag': 'PN|UTR|SIN|DEF|SUB', 'parent': '12'}, {'identifier': '12', 'word': 'älskar', 'base_word': 'älska', 'tag': 'VB|PRS|AKT', 'parent': '10'}, {'identifier': '13', 'word': 'henne', 'base_word': 'hon', 'tag': 'PN|UTR|SIN|DEF|OBJ', 'parent': '12'}, {'identifier': '14', 'word': 'med', 'base_word': 'med', 'tag': 'PP', 'parent': '17'}, {'identifier': '15', 'word': 'hela', 'base_word': 'hel', 'tag': 'JJ|POS|UTR/NEU|SIN|DEF|NOM', 'parent': '17'}, {'identifier': '16', 'word': 'sitt', 'base_word': 'sin', 'tag': 'PS|NEU|SIN|DEF', 'parent': '17'}, {'identifier': '17', 'word': 'hjärta', 'base_word': 'hjärta', 'tag': 'NN|NEU|SIN|IND|NOM', 'parent': '12'}]}\n",
      "All words:\n",
      "['han', 'ler', 'mot', 'henne', 'och', 'hela', 'hans', 'ansikte', 'säger', 'att', 'han', 'älskar', 'henne', 'med', 'hela', 'sitt', 'hjärta']\n",
      "Level three data:\n",
      "['han::PN', 'ler::VB', 'mot::PP', 'henne::PN', 'säger::VB']\n",
      "Only nouns and verbs:\n",
      "['ler', 'ansikte', 'säger', 'älskar', 'hjärta']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example filtering\n",
    "print (\"Raw data:\")\n",
    "print (res)\n",
    "print (\"All words:\")\n",
    "print ([word['word'] for word in res['sentenceData']])\n",
    "print (\"Level three data:\")\n",
    "print ([word['word']+ '::' + word['tag'].split('|')[0] for word in res['sentenceData'] if (int)(word['parent']) <= 3])\n",
    "print (\"Only nouns and verbs:\")\n",
    "print ([word['word'] for word in res['sentenceData'] if 'NN' in word['tag'].split('|') or 'VB' in word['tag'].split('|')])\n",
    "\n",
    "print(\" \".join(toWordArray(untilLevel(3, onlyNounsAndVerbs(parseSentence(res))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desktop-godesity\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "loadLibFolder('../gensim')\n",
    "\n",
    "import os\n",
    "import gensim\n",
    "import gensim_documents\n",
    "import dotenv\n",
    "import numpy as np\n",
    "dotenv.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit_per_category = 1000\n",
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New epoch started, nr. 1  of  7  epochs\n",
      "New epoch started, nr. 2  of  7  epochs\n",
      "New epoch started, nr. 3  of  7  epochs\n",
      "New epoch started, nr. 4  of  7  epochs\n",
      "New epoch started, nr. 5  of  7  epochs\n",
      "New epoch started, nr. 6  of  7  epochs\n"
     ]
    }
   ],
   "source": [
    "categories = []\n",
    "x_data = []\n",
    "y_data = []\n",
    "model = gensim.models.Doc2Vec.load(dotenv.get('DOC2VEC_MODEL'))\n",
    "\n",
    "if use_cache and os.path.isfile('data/tmp_dependency_data'):\n",
    "    with open('data/tmp_dependency_data', 'r', encoding='utf-8', errors='ignore') as tmp_cache_file:\n",
    "        for category in tmp_cache_file:\n",
    "            category = category[:-1]\n",
    "            if category == \"\\n\": continue\n",
    "            if category not in categories:\n",
    "                print (\"TT\", category)\n",
    "                categories.append(category)\n",
    "            sentVecs = []\n",
    "            while True:\n",
    "                sentence = tmp_cache_file.readline()[:-1]\n",
    "                if sentence == \"\":\n",
    "                    break\n",
    "                artvec = model.infer_vector(doc_words=sentence.split())\n",
    "                sentVecs.append(gensim.matutils.unitvec(artvec))\n",
    "            y_data.append(categories.index(category))\n",
    "            x_data.append(sentVecs)\n",
    "else:\n",
    "    data = gensim_documents.MMDBDocumentLists(dotenv.get('ARTICLE_PATH', '.') + '/csv_by_category/', useHeading=True, limit=limit_per_category)\n",
    "    with open('data/tmp_dependency_data', 'w', encoding='utf-8', errors='ignore') as tmp_cache_file:\n",
    "        for i, doc in enumerate(data):\n",
    "            if not doc.category in categories:\n",
    "                categories.append(doc.category)\n",
    "            tmp_cache_file.write(doc.category + \"\\n\")\n",
    "\n",
    "            sentences = doc.content.split(\".\")\n",
    "            sentVecs = []\n",
    "            for j in range(20):\n",
    "                if j >= len(sentences): \n",
    "                    sentVecs.append(np.zeros(300))\n",
    "                    continue\n",
    "                sentence = \" \".join(toWordArray(untilLevel(3, onlyNounsAndVerbs(parseSentence(sentences[j])))))\n",
    "                if sentence == \"\":\n",
    "                    sentVecs.append(np.zeros(300))\n",
    "                    continue\n",
    "                artvec = model.infer_vector(doc_words=sentence.split())\n",
    "                sentVecs.append(gensim.matutils.unitvec(artvec))\n",
    "                tmp_cache_file.write(sentence + \"\\n\")\n",
    "            tmp_cache_file.write(\"\\n\")\n",
    "            x_data.append(sentVecs)\n",
    "            y_data.append(categories.index(doc.category))\n",
    "\n",
    "            if i % limit_per_category == 0:\n",
    "                print (\"New epoch started, nr.\", i+1, \" of \", len(categories) * limit_per_category, \" epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode one hot vectors for the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_one_hot = np.zeros((len(y_data), len(categories)))\n",
    "y_data_one_hot[np.arange(len(y_data)), np.array(y_data)] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM classification with keras LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim = len(x_data[0][0])\n",
    "timesteps = len(x_data[0])\n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 0.4\n",
    "# Generate dummy training data\n",
    "x_train = x_data[:(int)(len(x_data) * split)]\n",
    "y_train = y_data_one_hot[:(int)(len(x_data) * split)]\n",
    "\n",
    "# Generate dummy validation data\n",
    "x_val = x_data[(int)(len(x_data) * split):]\n",
    "y_val = y_data_one_hot[(int)(len(x_data) * split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "20\n",
      "300\n",
      "16\n",
      "20\n",
      "300\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_val))\n",
    "print(len(x_val[0]))\n",
    "print(len(x_val[0][0]))\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_train[0]))\n",
    "print(len(x_train[0][0]))\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a Sequential LSTM model that can classify a stacked sequence of words we need to define the input as follows:\n",
    " * batch_size - number of datapoints in the dataset\n",
    " * timesteps - the number of words per sequence\n",
    " * data_dim - the number of features per word instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16 samples, validate on 26 samples\n",
      "Epoch 1/5\n",
      "16/16 [==============================] - 8s - loss: 1.9467 - acc: 0.1250 - val_loss: 1.9460 - val_acc: 0.2692\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - 8s - loss: 1.9347 - acc: 0.4375 - val_loss: 1.9467 - val_acc: 0.2692\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - 8s - loss: 1.9227 - acc: 0.5000 - val_loss: 1.9483 - val_acc: 0.2692\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - 8s - loss: 1.9058 - acc: 0.3750 - val_loss: 1.9521 - val_acc: 0.3077\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - 8s - loss: 1.8808 - acc: 0.3125 - val_loss: 1.9589 - val_acc: 0.3077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x258ef4ad588>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(32))  # return a single vector of dimension 32\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9588522911071777, 0.30769231915473938]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
