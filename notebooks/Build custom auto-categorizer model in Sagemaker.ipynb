{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Auto categorizer model in Sagemaker\n",
    "<hr>\n",
    "<div style=\"background: #fff;display:inline-block: color: #222;\">\n",
    "<img src=\"old-work-overview.png\" style=\"display:inline-block;max-width:40%;max-height:100%;\" /><img src=\"auto-categorizer-model.png\" style=\"display:inline-block;max-width: 60%;max-height:40vh;\" />\n",
    "</div>\n",
    "Sagemaker consists of six step deplyment. To deploy a model trained by the network shown in the picture, follow this notebook guide. Step one is to create a notebook that have the overview of the deployement (copy of this notebook), step two is to upload training/testing data to s3, step three is to create a ECS container with the algorithm, step four is to create a training job that will launch the container and train a model, step five is to deploy the model as an endpoint and step six is to use a predictor (alternative HTTP requests with signed AWS key) to use the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `container/build-and-push.sh`, which you can run as `build-and-push.sh sagemaker-auto-categorization` to build the image `sagemaker-auto-categorization`. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon    636MB\n",
      "Step 1/19 : FROM ubuntu:16.04\n",
      " ---> 7e87e2b3bf7a\n",
      "Step 2/19 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> 8fbda223875c\n",
      "Step 3/19 : WORKDIR /opt\n",
      " ---> Using cache\n",
      " ---> b5e579200e0a\n",
      "Step 4/19 : COPY requirements.txt /opt/requirements.txt\n",
      " ---> Using cache\n",
      " ---> f17aeedf6f36\n",
      "Step 5/19 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3          nginx          git          libicu-dev          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 0a15d4a4f323\n",
      "Step 6/19 : RUN alias python=python3\n",
      " ---> Using cache\n",
      " ---> 51d3395c87c4\n",
      "Step 7/19 : RUN wget https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py &&     pip install -r requirements.txt &&         rm -rf /root/.cache\n",
      " ---> Using cache\n",
      " ---> 55b8d0a4e299\n",
      "Step 8/19 : ENV MODEL_PATH=/opt/ml/model\n",
      " ---> Using cache\n",
      " ---> dc8f06a791be\n",
      "Step 9/19 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> c3879771705b\n",
      "Step 10/19 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 9e8957fabb97\n",
      "Step 11/19 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> e82dc71ef3e3\n",
      "Step 12/19 : ENV ENV=sagemaker\n",
      " ---> Using cache\n",
      " ---> 96c7bf6df949\n",
      "Step 13/19 : RUN rm requirements.txt get-pip.py\n",
      " ---> Using cache\n",
      " ---> 9039e7f3c15f\n",
      "Step 14/19 : COPY web /opt/program/web\n",
      " ---> Using cache\n",
      " ---> dcb2037bb346\n",
      "Step 15/19 : COPY learning /opt/program/learning\n",
      "The push refers to repository [682250509414.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-auto-categorization]\n",
      "7c776ca613eb: Preparing\n",
      "cbd9ca09287b: Preparing\n",
      "48c118956c24: Preparing\n",
      "ee0100bf9ab8: Preparing\n",
      "b0b5f6fb5bb8: Preparing\n",
      "76e31168e142: Preparing\n",
      "c3112f6d73bb: Preparing\n",
      "d400f22affc4: Preparing\n",
      "68dda0c9a8cd: Preparing\n",
      "f67191ae09b8: Preparing\n",
      "b2fd8b4c3da7: Preparing\n",
      "0de2edf7bff4: Preparing\n",
      "c3112f6d73bb: Waiting\n",
      "d400f22affc4: Waiting\n",
      "68dda0c9a8cd: Waiting\n",
      "f67191ae09b8: Waiting\n",
      "b2fd8b4c3da7: Waiting\n",
      "0de2edf7bff4: Waiting\n",
      "76e31168e142: Waiting\n",
      "b0b5f6fb5bb8: Layer already exists\n",
      "ee0100bf9ab8: Layer already exists\n",
      "cbd9ca09287b: Layer already exists\n",
      "48c118956c24: Layer already exists\n",
      "7c776ca613eb: Layer already exists\n",
      "76e31168e142: Layer already exists\n",
      "c3112f6d73bb: Layer already exists\n",
      "d400f22affc4: Layer already exists\n",
      "b2fd8b4c3da7: Layer already exists\n",
      "68dda0c9a8cd: Layer already exists\n",
      "f67191ae09b8: Layer already exists\n",
      "0de2edf7bff4: Layer already exists\n",
      "latest: digest: sha256:ef37d00194c3bd16f1e12099dea499feeebd5fd055901f033f76b34a3715086a size: 2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Error processing tar file(exit status 1): write /opt/program/learning/trained-models/doc2vec.model.docvecs.doctag_syn0.npy: no space left on device\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-auto-categorization\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} -f ../_sagemaker/Dockerfile ../ \n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In the container the input and output folders is defined by sagemaker...\n",
    "Sagemaker service automatically mounts the S3 training-data onto the container into some predetermined folders. In the same way sagemaker expects to find the resulting model and other things at a predefined output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### The input\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how your program runs. `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. `resourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn't support distributed training, we'll ignore it here.\n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it's generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. \n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### The output\n",
    "\n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is a directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file will be returned in the `FailureReason` field of the `DescribeTrainingJob` result. For jobs that succeed, there is no reason to write this file as it will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 2: Training and Hosting your Algorithm in Amazon SageMaker\n",
    "\n",
    "Once you have your container packaged, you can use it to train and serve models. Let's do that with the algorithm we made above.\n",
    "\n",
    "## Set up the environment\n",
    "\n",
    "Here we specify a bucket to use and the role that will be used for working with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'data/DEMO-auto-categorizer'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker.predictor import json_serializer\n",
    "import requests\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Upload the data for training\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some articles from CS (AWS RDS). \n",
    "\n",
    "We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = '../learning/data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-auto-categorization:latest'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c4.8xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train model\n",
    "To start a training job that might take a long time to finnish make sure that all settings for the model are set up and working beforehand. It helps if you've tried it locally before running it at AWS sagemaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-auto-categorization-2019-03-05-15-00-35-197\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deploy the model\n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-auto-categorization-2019-02-11-14-32-56-853\n",
      "INFO:sagemaker:Creating endpoint with name categorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=json_serializer, endpoint_name='categorize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In case the notebook is restarted it is easy to access the deployed model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictor2 = sage.predictor.RealTimePredictor('auto-categorize', sess, serializer=json_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose some data and use it for a prediction\n",
    "\n",
    "In order to do some predictions, we'll test the algorithm on some of the data we used for training and do predictions against it. This is, of course, a bad way to evaluate the model, but a good way to see how the mechanism works and how it can be used in production. We will use both the Sagemaker API and a simple HTTP request to the endpoint that will do the same things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'body': \"\"\"Nu börjar en fullskalig strejk för alla som är medlemmar i Hamnarbetarförbundet i Sundsvalls hamn och vid många andra hamnar i Sverige. Hamnarbetarförbundets syfte med strejken är att få igenom ett rikstäckande kollektivavtal med arbetsgivarorganisationen Sveriges Hamnar. – 90 procent av allt fackligt arbete sker lokalt, det känns som en självklarhet att det ska finnas kollektivavtal för våra medlemmar, säger Henrik Henriksson. Men arbetsgivarorganisationen menar att det redan finns ett kollektivavtal tecknat med Transportarbetareförbundet och de vill erbjuda samma avtal till Hamnarbetarförbundets medlemmar. – Det finns redan ett kollektivavtal, det är inte rätt att arbetare på samma arbetsplats kan ha olika villkor, säger Björn Lyngfelt, kommunikationsdirektör på SCA.\"\"\",\n",
    "    'categories2': None,\n",
    "    'uuid': None\n",
    "}\n",
    "predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"categoreis\": {\"Ekonomi, n\\u00e4ringsliv & finans\": 0.08338597416877747, \"Politik\": 0.15729863941669464, \"Brott & straff\": 0.03802034631371498, \"Sport\": 0.03394358977675438, \"Personligt\": 0.10776830464601517, \"Olyckor & katastrofer\": 0.03849613294005394, \"V\\u00e4der\": 0.042467325925827026, \"Livsstil & fritid\": 0.18372845649719238, \"Samh\\u00e4lle & v\\u00e4lf\\u00e4rd\": 0.2315531075000763, \"Kultur & n\\u00f6je\": 0.08333814889192581}, \"category\": {\"category_name\": \"Samh\\u00e4lle & v\\u00e4lf\\u00e4rd\", \"category_probability\": 0.2315531075000763}, \"classified_text\": \"Nu b\\u00f6rjar en fullskalig strejk f\\u00f6r alla som \\u00e4r medlemmar i Hamnarbetarf\\u00f6rbundet i Sundsvalls hamn och vid m\\u00e5nga andra hamnar i Sverige.\\n\\nHamnarbetarf\\u00f6rbundets syfte med strejken \\u00e4r att f\\u00e5 igenom ett rikst\\u00e4ckande kollektivavtal med arbetsgivarorganisationen Sveriges Hamnar.\\n\\n\\u2013 90 procent av allt fackligt arbete sker lokalt, det k\\u00e4nns som en sj\\u00e4lvklarhet att det ska finnas kollektivavtal f\\u00f6r v\\u00e5ra medlemmar, s\\u00e4ger Henrik Henriksson.\\n\\nMen arbetsgivarorganisationen menar att det redan finns ett kollektivavtal tecknat med Transportarbetaref\\u00f6rbundet och de vill erbjuda samma avtal till Hamnarbetarf\\u00f6rbundets medlemmar.\\n\\n\\u2013 Det finns redan ett kollektivavtal, det \\u00e4r inte r\\u00e4tt att arbetare p\\u00e5 samma arbetsplats kan ha olika villkor, s\\u00e4ger Bj\\u00f6rn Lyngfelt, kommunikationsdirekt\\u00f6r p\\u00e5 SCA.\\\"\\\"\\\"\\n\", \"entities\": []}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://runtime.sagemaker.eu-west-1.amazonaws.com/endpoints/auto-categorize/invocations\"\n",
    "payload = \"Nu börjar en fullskalig strejk för alla som är medlemmar i Hamnarbetarförbundet i Sundsvalls hamn och vid många andra hamnar i Sverige.\"\n",
    "headers = {\n",
    "    'content-type': \"text/csv\",\n",
    "    'host': \"runtime.sagemaker.eu-west-1.amazonaws.com\",\n",
    "    'x-amz-date': \"20190402T160957Z\",\n",
    "    'authorization': \"AWS4-HMAC-SHA256 Credential=AKIAJ5OU7RYJ2G4TUO5A/20190402/eu-west-1/sagemaker/aws4_request, SignedHeaders=content-length;content-type;host;x-amz-date, Signature=8e76fc7258b2266e4838361616c756773ea6cc879f124a1665e3443baed8ff54\",\n",
    "    'cache-control': \"no-cache\" }\n",
    "requests.request(\"POST\", url, data=payload.encode('utf-8'), headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optional cleanup\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-auto-categorization-2018-09-28-10-58-26-758\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sagemaker general cleanup\n",
    "In order to keep costs down and as few services running at the same time these are some scripts to quickly remove unused resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import boto3 # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html\n",
    "client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Stop all training jobs\n",
    "for job in client.list_training_jobs()['TrainingJobSummaries']:\n",
    "    if job['TrainingJobStatus'] == 'Running':\n",
    "        client.stop_training_job(TrainingJobName=job['TrainingJobName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Delete all failed endpoints\n",
    "for job in client.list_endpoints()['Endpoints']:\n",
    "    if job['EndpointStatus'] != 'InService':\n",
    "        client.delete_endpoint(EndpointName=job['EndpointName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Delete all endpoint configs\n",
    "for job in client.list_endpoint_configs()['EndpointConfigs']:\n",
    "    client.delete_endpoint_configs(EndpointConfigName=job['EndpointConfigName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Delete all models\n",
    "for job in client.list_models()['Models']:\n",
    "    client.delete_model(ModelName=bjob['ModelName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
