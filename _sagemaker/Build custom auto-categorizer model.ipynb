{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto categorizer model in Sagemaker\n",
    "\n",
    "<img src=\"old-work-overview.png\" width=\"40%\" style=\"float:left;\" /><img src=\"auto-categorizer-model.png\" width=\"60%\" style=\"float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and registering the container\n",
    "\n",
    "The following shell code shows how to build the container image using `docker build` and push the container image to ECR using `docker push`. This code is also available as the shell script `container/build-and-push.sh`, which you can run as `build-and-push.sh sagemaker-auto-categorization` to build the image `sagemaker-auto-categorization`. \n",
    "\n",
    "This code looks for an ECR repository in the account you're using and the current default region (if you're using a SageMaker notebook instance, this will be the region where the notebook instance was created). If the repository doesn't exist, the script will create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  636.3MB\n",
      "Step 1/19 : FROM ubuntu:16.04\n",
      " ---> 9361ce633ff1\n",
      "Step 2/19 : MAINTAINER Amazon AI <sage-learner@amazon.com>\n",
      " ---> Using cache\n",
      " ---> 4b7374d7056b\n",
      "Step 3/19 : WORKDIR /opt\n",
      " ---> Using cache\n",
      " ---> b8b1b0df6f50\n",
      "Step 4/19 : COPY requirements.txt /opt/requirements.txt\n",
      " ---> Using cache\n",
      " ---> bf7fce3ee0d6\n",
      "Step 5/19 : RUN apt-get -y update && apt-get install -y --no-install-recommends          wget          python3          nginx          python-gevent python-gevent-websocket          git          libicu-dev          ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> aae235f86bf7\n",
      "Step 6/19 : RUN alias python=python3\n",
      " ---> Using cache\n",
      " ---> aa86a305c578\n",
      "Step 7/19 : RUN wget https://bootstrap.pypa.io/get-pip.py && python3 get-pip.py &&     pip install -r requirements.txt &&         rm -rf /root/.cache\n",
      " ---> Using cache\n",
      " ---> 401890143c22\n",
      "Step 8/19 : ENV MODEL_PATH=/opt/ml/model\n",
      " ---> Using cache\n",
      " ---> d4c080fca92b\n",
      "Step 9/19 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 1e0c077a4d15\n",
      "Step 10/19 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> bbc77fe03c87\n",
      "Step 11/19 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 297a317461cb\n",
      "Step 12/19 : ENV ENV=sagemaker\n",
      " ---> Using cache\n",
      " ---> 21fa39f841d1\n",
      "Step 13/19 : RUN rm requirements.txt get-pip.py\n",
      " ---> Using cache\n",
      " ---> 151dd87ce6f6\n",
      "Step 14/19 : COPY web /opt/program/web\n",
      " ---> 3a1a4ea9f0cf\n",
      "Step 15/19 : COPY learning /opt/program/learning\n",
      " ---> 3c0eb60315f6\n",
      "Step 16/19 : COPY config /opt/program/config\n",
      " ---> ea570f79f063\n",
      "Step 17/19 : COPY _sagemaker/sagemaker.py /opt/program/sagemaker.py\n",
      " ---> 6e1dff8560f1\n",
      "Step 18/19 : WORKDIR /opt/program\n",
      "Removing intermediate container c25eff5f186a\n",
      " ---> 278a3c35b6df\n",
      "Step 19/19 : ENTRYPOINT [\"python3\", \"sagemaker.py\"]\n",
      " ---> Running in 07870350f251\n",
      "Removing intermediate container 07870350f251\n",
      " ---> 0792d555d844\n",
      "Successfully built 0792d555d844\n",
      "Successfully tagged sagemaker-auto-categorization:latest\n",
      "The push refers to repository [682250509414.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-auto-categorization]\n",
      "4ad679b021e9: Preparing\n",
      "57e06f09668f: Preparing\n",
      "db04b2194fdc: Preparing\n",
      "d5792bcdc81b: Preparing\n",
      "66d8308fb0ff: Preparing\n",
      "7d6dd5ad8d74: Preparing\n",
      "473d17ae607d: Preparing\n",
      "b8b148dca4e0: Preparing\n",
      "297fd071ca2f: Preparing\n",
      "2f0d1e8214b2: Preparing\n",
      "7dd604ffa87f: Preparing\n",
      "aa54c2bc1229: Preparing\n",
      "b8b148dca4e0: Waiting\n",
      "297fd071ca2f: Waiting\n",
      "2f0d1e8214b2: Waiting\n",
      "7dd604ffa87f: Waiting\n",
      "aa54c2bc1229: Waiting\n",
      "7d6dd5ad8d74: Waiting\n",
      "473d17ae607d: Waiting\n",
      "66d8308fb0ff: Layer already exists\n",
      "7d6dd5ad8d74: Layer already exists\n",
      "473d17ae607d: Layer already exists\n",
      "b8b148dca4e0: Layer already exists\n",
      "297fd071ca2f: Layer already exists\n",
      "2f0d1e8214b2: Layer already exists\n",
      "7dd604ffa87f: Layer already exists\n",
      "aa54c2bc1229: Layer already exists\n",
      "4ad679b021e9: Pushed\n",
      "d5792bcdc81b: Pushed\n",
      "57e06f09668f: Pushed\n",
      "db04b2194fdc: Pushed\n",
      "latest: digest: sha256:aed61262337f216a86147fd1078550e9f501d184469352cf17dabb9507529de0 size: 2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-auto-categorization\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} -f _sagemaker/Dockerfile . \n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the container the input and output is defined by sagemaker like the following:\n",
    "###### The input\n",
    "\n",
    "* `/opt/ml/input/config` contains information to control how your program runs. `hyperparameters.json` is a JSON-formatted dictionary of hyperparameter names to values. These values will always be strings, so you may need to convert them. `resourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training. Since scikit-learn doesn't support distributed training, we'll ignore it here.\n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to CreateTrainingJob but it's generally important that channels match what the algorithm expects. The files for each channel will be copied from S3 to this directory, preserving the tree structure indicated by the S3 key structure. \n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and go up by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.\n",
    "\n",
    "###### The output\n",
    "\n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format that you want. It can be a single file or a whole directory tree. SageMaker will package any files in this directory into a compressed tar archive file. This file will be available at the S3 location returned in the `DescribeTrainingJob` result.\n",
    "* `/opt/ml/output` is a directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file will be returned in the `FailureReason` field of the `DescribeTrainingJob` result. For jobs that succeed, there is no reason to write this file as it will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training and Hosting your Algorithm in Amazon SageMaker\n",
    "\n",
    "Once you have your container packaged, you can use it to train and serve models. Let's do that with the algorithm we made above.\n",
    "\n",
    "## Set up the environment\n",
    "\n",
    "Here we specify a bucket to use and the role that will be used for working with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'data/DEMO-auto-categorizer'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the session\n",
    "\n",
    "The session remembers our connection parameters to SageMaker. We'll use it to perform all of our SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some articles from CS (AWS RDS). \n",
    "\n",
    "We can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = 'learning/data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator and fit the model\n",
    "\n",
    "In order to use SageMaker to fit our algorithm, we'll create an `Estimator` that defines how to use the container to train. This includes the configuration we need to invoke SageMaker training:\n",
    "\n",
    "* The __container name__. This is constructed as in the shell commands above.\n",
    "* The __role__. As defined above.\n",
    "* The __instance count__ which is the number of machines to use for training.\n",
    "* The __instance type__ which is the type of machine to use for training.\n",
    "* The __output path__ determines where the model artifact will be written.\n",
    "* The __session__ is the SageMaker session object that we defined above.\n",
    "\n",
    "Then we use fit() on the estimator to train against the data that we uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-auto-categorization:latest'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c4.8xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-auto-categorization-2019-03-26-19-21-04-297\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# ^ Catpure the output of fit so that the output will make this a very large file when epoch > 10\n",
    "tree.fit(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Deploying the model to SageMaker hosting just requires a `deploy` call on the fitted model. This call takes an instance count, instance type, and optionally serializer and deserializer functions. These are used when the resulting predictor is created on the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-auto-categorization-2019-03-26-20-02-11-774\n",
      "INFO:sagemaker:Creating endpoint with name auto-categorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = tree.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer, endpoint_name='auto-categorize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose some data and use it for a prediction\n",
    "\n",
    "In order to do some predictions, we'll test the algorithm on some of the data we used for training and do predictions against it. This is, of course, bad statistical practice, but a good way to see how the mechanism works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'accept',\n",
       " 'content_type',\n",
       " 'delete_endpoint',\n",
       " 'deserializer',\n",
       " 'endpoint',\n",
       " 'predict',\n",
       " 'sagemaker_session',\n",
       " 'serializer']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'categoreis': {'Ekonomi, näringsliv & finans': 0.9883291721343994,\n",
       "  'Politik': 0.0004124774714000523,\n",
       "  'Brott & straff': 0.0005652520922012627,\n",
       "  'Sport': 2.961635254905559e-05,\n",
       "  'Personligt': 0.003377333516255021,\n",
       "  'Olyckor & katastrofer': 7.904069207143039e-05,\n",
       "  'Väder': 1.1770806622735108e-06,\n",
       "  'Livsstil & fritid': 0.0017547460738569498,\n",
       "  'Samhälle & välfärd': 0.002122658072039485,\n",
       "  'Kultur & nöje': 0.003328553168103099},\n",
       " 'category': {'category_name': 'Ekonomi, näringsliv & finans',\n",
       "  'category_probability': 0.9883291721343994},\n",
       " 'classified_text': '\\n\\nEnligt sin egen beskrivning ska Kirtap AB ägna sig åt \"hotell- och restaurangverksamhet, äga och förvalta fastigheter\".\\n\\nBolaget, som registrerades hos Bolagsverket den 22 mars, har sitt säte i Sundsvall.\\n\\nKirtap AB har ett aktiekapital på 50 000 kronor.\\n\\nStyrelseledamot är Patrik Attini, 44 år.\\n\\n',\n",
       " 'entities': []}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "payload = \"\"\"\n",
    "\n",
    "Enligt sin egen beskrivning ska Kirtap AB ägna sig åt \"hotell- och restaurangverksamhet, äga och förvalta fastigheter\".\n",
    "\n",
    "Bolaget, som registrerades hos Bolagsverket den 22 mars, har sitt säte i Sundsvall.\n",
    "\n",
    "Kirtap AB har ett aktiekapital på 50 000 kronor.\n",
    "\n",
    "Styrelseledamot är Patrik Attini, 44 år.\n",
    "\n",
    "\"\"\"\n",
    "json.loads(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional cleanup\n",
    "\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-auto-categorization-2018-09-28-10-58-26-758\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
